{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork or copy of my previous developed notebook for digit recognition. Therefore you will find some parts that look common to the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> or <a href=\"https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\">Finetuning Hyperparameters in Deep Neural Network</a>.\n",
    "\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "The current best run was based on the Version 7 with an accuracy of 0.97657 on the kaggle competition \"Digit Recognzier\"\n",
    "\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Finetuning Hyperparameters in Deep Neural Network:\n",
    "    - https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\n",
    "- Digit Recognition with a Deep Neural Network: \n",
    "    - https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try:\n",
    "    - https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters:\n",
    "    - https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "\n",
    "    ## final solution\n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x000001E901A0E9D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x000001E901A0E9D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_11_12-17_24_34\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_11_12-17_24_34'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28,1]\n",
    "\n",
    "batch_shape = []\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None\n",
    "\n",
    "# Model building\n",
    "def create_model_struc():  \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n",
    "    model.build()\n",
    "\n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 80,394\n",
      "Trainable params: 80,010\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_struc()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "   2/2100 [..............................] - ETA: 7:19 - loss: 3.3248 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.4090s). Check your callbacks.\n",
      "2100/2100 [==============================] - 22s 11ms/step - loss: 0.1973 - accuracy: 0.9401 - val_loss: 0.1185 - val_accuracy: 0.9623\n",
      "Epoch 2/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0882 - accuracy: 0.9722 - val_loss: 0.0991 - val_accuracy: 0.9709\n",
      "Epoch 3/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0660 - accuracy: 0.9787 - val_loss: 0.0961 - val_accuracy: 0.9704\n",
      "Epoch 4/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0523 - accuracy: 0.9836 - val_loss: 0.1110 - val_accuracy: 0.9667\n",
      "Epoch 5/14\n",
      "2100/2100 [==============================] - 22s 11ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.0763 - val_accuracy: 0.9766\n",
      "Epoch 6/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0355 - accuracy: 0.9880 - val_loss: 0.0930 - val_accuracy: 0.9722\n",
      "Epoch 7/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0297 - accuracy: 0.9900 - val_loss: 0.0794 - val_accuracy: 0.9782\n",
      "Epoch 8/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0242 - accuracy: 0.9916 - val_loss: 0.0679 - val_accuracy: 0.9806\n",
      "Epoch 9/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0193 - accuracy: 0.9934 - val_loss: 0.0759 - val_accuracy: 0.9802\n",
      "Epoch 10/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0164 - accuracy: 0.9944 - val_loss: 0.0776 - val_accuracy: 0.9801\n",
      "Epoch 11/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.0779 - val_accuracy: 0.9789\n",
      "Epoch 12/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0735 - val_accuracy: 0.9827\n",
      "Epoch 13/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0141 - accuracy: 0.9951 - val_loss: 0.0738 - val_accuracy: 0.9815\n",
      "Epoch 14/14\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0728 - val_accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=14, validation_data=val_ds, callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=20), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 80,394\n",
      "Trainable params: 80,010\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full = create_model_struc()\n",
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "   2/2625 [..............................] - ETA: 10:23 - loss: 3.3984 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.4661s). Check your callbacks.\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.1792 - accuracy: 0.9453\n",
      "Epoch 2/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0815 - accuracy: 0.9746\n",
      "Epoch 3/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0613 - accuracy: 0.9808\n",
      "Epoch 4/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0483 - accuracy: 0.9843\n",
      "Epoch 5/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0395 - accuracy: 0.9873\n",
      "Epoch 6/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0337 - accuracy: 0.9889\n",
      "Epoch 7/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0280 - accuracy: 0.9910\n",
      "Epoch 8/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0234 - accuracy: 0.9921\n",
      "Epoch 9/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0210 - accuracy: 0.9930\n",
      "Epoch 10/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0173 - accuracy: 0.9941\n",
      "Epoch 11/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0152 - accuracy: 0.9949\n",
      "Epoch 12/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0140 - accuracy: 0.9955\n",
      "Epoch 13/14\n",
      "2625/2625 [==============================] - 28s 11ms/step - loss: 0.0127 - accuracy: 0.9957\n",
      "Epoch 14/14\n",
      "2625/2625 [==============================] - 28s 11ms/step - loss: 0.0112 - accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=14, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [3.6760672e-19 2.7813120e-22 1.0000000e+00 4.9501728e-15 1.5551702e-16\n",
      " 4.5903582e-19 1.1105674e-20 1.1551887e-16 6.7895241e-18 7.3449477e-19]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae09ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
