{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55c3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03773a7f",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork or copy of my previous developed notebook for digit recognition. Therefore you will find some parts that look common to the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> or <a href=\"https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\">Finetuning Hyperparameters in Deep Neural Network</a>.\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "My focus on this notebook lies in using Convolutional Neural Networks. I worked with them before but since I read different articles and books about its architecture, I got a deeper understanding of the different layers and their result to the rest of the network. \n",
    "\n",
    "As in the previous notebooks I will use different architecture / layer configurations and submit the results to the Kaggle competition to get a rated accuracy value. This will be an indicator for the used model architecture. The plan is to commit the notebook in Git as well as in Kaggle to versionize the architecture with its accuracy value. This will help me to understand the benefits of the different layers a little and look into the progress of the different architectures later.\n",
    "\n",
    "The idea is to use different layers in different combinations. The following layers will be used in this notebook:\n",
    "\n",
    "Convolutional layers (Conv2D, Conv3D,...)\n",
    "Max Pooling layers\n",
    "Avg Pooling  \n",
    "Batch Normalization\n",
    "Dropout\n",
    "\n",
    "\n",
    "Not part of this notebook will be the architecture of Transfer-Learning where a pretrained model is used and retrained with a new dataset. I already tried that approach in the following notebook: https://www.kaggle.com/skiplik/picturerecognition-tf-and-transferlearning-resnet\n",
    "\n",
    "## Best Run\n",
    "The best run was based on Kaggle version 6 with an accuracy of 99.12% on the kaggle competition \"Digit Recognizer\". For this version the special improvement was (next to the Conv2D layers, the reduction of the epochs for training): https://www.kaggle.com/skiplik/cnn-for-digit-recognition-mnist?scriptVersionId=79696075\n",
    "\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Finetuning Hyperparameters in Deep Neural Network:\n",
    "    - https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\n",
    "- Digit Recognition with a Deep Neural Network: \n",
    "    - https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try:\n",
    "    - https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters:\n",
    "    - https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b10659",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee3cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e48d5",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4345f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998993c",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3873e566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22da4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd32c7",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba0f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7160ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0ad22",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f365352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeef7ff",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988c85b",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83d5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "\n",
    "    ## final solution\n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899fafd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cab30ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8f5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12f699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x000001D712B71CA0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x000001D712B71CA0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4b5519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa4ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b2e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8e4e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9edabf",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c859486",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7332d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2d5fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_11_16-17_15_26\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c880e991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_11_16-17_15_26'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11325f2",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b49fca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccd2e6",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac61847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28,1]\n",
    "\n",
    "batch_shape = []\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None\n",
    "\n",
    "# Model building\n",
    "def create_model_struc():  \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n",
    "    model.build()\n",
    "\n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed905797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 80,394\n",
      "Trainable params: 80,010\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_struc()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674dcbb",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fceea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bdf28f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fd797c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/2100 [..............................] - ETA: 0s - loss: 3.7807 - accuracy: 0.0938WARNING:tensorflow:From D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0100s vs `on_train_batch_end` time: 0.0290s). Check your callbacks.\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.1957 - accuracy: 0.9409 - val_loss: 0.1188 - val_accuracy: 0.9628\n",
      "Epoch 2/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0946 - accuracy: 0.9705 - val_loss: 0.0894 - val_accuracy: 0.9721\n",
      "Epoch 3/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0742 - accuracy: 0.9765 - val_loss: 0.0806 - val_accuracy: 0.9746\n",
      "Epoch 4/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0650 - accuracy: 0.9798 - val_loss: 0.1418 - val_accuracy: 0.9577\n",
      "Epoch 5/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0544 - accuracy: 0.9827 - val_loss: 0.0764 - val_accuracy: 0.9761\n",
      "Epoch 6/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0506 - accuracy: 0.9841 - val_loss: 0.0688 - val_accuracy: 0.9783\n",
      "Epoch 7/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0470 - accuracy: 0.9857 - val_loss: 0.0899 - val_accuracy: 0.9758\n",
      "Epoch 8/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0435 - accuracy: 0.9869 - val_loss: 0.0899 - val_accuracy: 0.9724\n",
      "Epoch 9/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 0.0727 - val_accuracy: 0.9781\n",
      "Epoch 10/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0385 - accuracy: 0.9882 - val_loss: 0.0566 - val_accuracy: 0.9835\n",
      "Epoch 11/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 0.0568 - val_accuracy: 0.9835\n",
      "Epoch 12/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0370 - accuracy: 0.9886 - val_loss: 0.0567 - val_accuracy: 0.9820\n",
      "Epoch 13/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0957 - val_accuracy: 0.9724\n",
      "Epoch 14/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0531 - val_accuracy: 0.9841\n",
      "Epoch 15/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0324 - accuracy: 0.9901 - val_loss: 0.0585 - val_accuracy: 0.9842\n",
      "Epoch 16/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.0647 - val_accuracy: 0.9824\n",
      "Epoch 17/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0282 - accuracy: 0.9913 - val_loss: 0.0572 - val_accuracy: 0.9823\n",
      "Epoch 18/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0291 - accuracy: 0.9913 - val_loss: 0.0617 - val_accuracy: 0.9837\n",
      "Epoch 19/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 0.0512 - val_accuracy: 0.9861\n",
      "Epoch 20/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0265 - accuracy: 0.9919 - val_loss: 0.0997 - val_accuracy: 0.9750\n",
      "Epoch 21/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 0.0587 - val_accuracy: 0.9837\n",
      "Epoch 22/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0272 - accuracy: 0.9916 - val_loss: 0.0499 - val_accuracy: 0.9854\n",
      "Epoch 23/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0254 - accuracy: 0.9922 - val_loss: 0.0530 - val_accuracy: 0.9852\n",
      "Epoch 24/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.0680 - val_accuracy: 0.9824\n",
      "Epoch 25/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0236 - accuracy: 0.9926 - val_loss: 0.0495 - val_accuracy: 0.9860\n",
      "Epoch 26/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0236 - accuracy: 0.9931 - val_loss: 0.0521 - val_accuracy: 0.9853\n",
      "Epoch 27/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0232 - accuracy: 0.9930 - val_loss: 0.0506 - val_accuracy: 0.9852\n",
      "Epoch 28/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0230 - accuracy: 0.9933 - val_loss: 0.0517 - val_accuracy: 0.9860\n",
      "Epoch 29/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0242 - accuracy: 0.9928 - val_loss: 0.0498 - val_accuracy: 0.9862\n",
      "Epoch 30/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0208 - accuracy: 0.9936 - val_loss: 0.0580 - val_accuracy: 0.9848\n",
      "Epoch 31/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0214 - accuracy: 0.9937 - val_loss: 0.0610 - val_accuracy: 0.9829\n",
      "Epoch 32/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0225 - accuracy: 0.9933 - val_loss: 0.0463 - val_accuracy: 0.9869\n",
      "Epoch 33/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0616 - val_accuracy: 0.9813\n",
      "Epoch 34/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0558 - val_accuracy: 0.9846\n",
      "Epoch 35/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.0608 - val_accuracy: 0.9842\n",
      "Epoch 36/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.0469 - val_accuracy: 0.9865\n",
      "Epoch 37/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 0.0523 - val_accuracy: 0.9846\n",
      "Epoch 38/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.0489 - val_accuracy: 0.9856\n",
      "Epoch 39/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0193 - accuracy: 0.9943 - val_loss: 0.0471 - val_accuracy: 0.9875\n",
      "Epoch 40/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 0.0469 - val_accuracy: 0.9873\n",
      "Epoch 41/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0517 - val_accuracy: 0.9854\n",
      "Epoch 42/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.0424 - val_accuracy: 0.9886\n",
      "Epoch 43/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.0462 - val_accuracy: 0.9870\n",
      "Epoch 44/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 0.0394 - val_accuracy: 0.9889\n",
      "Epoch 45/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0430 - val_accuracy: 0.9879\n",
      "Epoch 46/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.0609 - val_accuracy: 0.9842\n",
      "Epoch 47/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0515 - val_accuracy: 0.9876\n",
      "Epoch 48/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0182 - accuracy: 0.9945 - val_loss: 0.0495 - val_accuracy: 0.9868\n",
      "Epoch 49/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.0507 - val_accuracy: 0.9868\n",
      "Epoch 50/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.0613 - val_accuracy: 0.9847\n",
      "Epoch 51/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 0.0522 - val_accuracy: 0.9858\n",
      "Epoch 52/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.0530 - val_accuracy: 0.9858\n",
      "Epoch 53/100\n",
      "2100/2100 [==============================] - 21s 10ms/step - loss: 0.0171 - accuracy: 0.9947 - val_loss: 0.0405 - val_accuracy: 0.9880\n",
      "Epoch 54/100\n",
      "2100/2100 [==============================] - 22s 10ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.0425 - val_accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=65, validation_data=val_ds, callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf39b12",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d70c5628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhmUlEQVR4nO3de5Ckd13v8ff36etc9zqbvW82dzZAQPaEEDSrRI4JKqtVlCdBFIOcmBK8nlKwTnn8gyqLc/R45JRAjBHjhSKiokQqiIqQyCFAdiXG3IBNyN43O3uZy85Md08/z/f88Tzd093TM9Ozmd3efvJ5pZ56nt+lu78z2fk8l76ZuyMiIr0v6HYBIiKyMhToIiIpoUAXEUkJBbqISEoo0EVEUiLbrQdev369X3755d16eBGRnrR///5T7j7SbqxrgX755Zezb9++bj28iEhPMrODC43pkouISEoo0EVEUkKBLiKSEgp0EZGUWDLQzewTZnbSzJ5aYNzM7P+a2QEze9LMvmflyxQRkaV0coT+AHDbIuO3A1cny93Ax19+WSIislxLBrq7PwqcWWTKXuDPPPY1YLWZbVqpAkVEpDMr8Tr0LcDhhvaRpO9460Qzu5v4KJ7t27evwEOLvIK5g0cQhfG67eKLjCVLu/uda7y8fo/Ak/qiqLndVHsYz29qt/nZGscBMDBbYB209NHBnIZ121p9fn9TXb7Az9eybL8JrnzLsv53d2IlAt3a9LX9kHV3vw+4D2D37t36IHZZniiCqArRbLIOIZxt6Avj7bBhvD63tkRz2x7O3aa+riZ/iNWW/jCZ39gfNtxPy3101O7wNp0GsfSON//yJRvoR4BtDe2twLEVuF85H1EUh1hYiYMtrMxtV8vt+5u2F5sz2xJsYUMoNoZbOH9e09yF5lUhbA28Wv8sCxwnXFxBFiwTr4MsBEHczuSSduNYm3YmB7m+hcdb25ZJHmOxxeJ5i44vMtZ4TFY7km3b16Kpf4H5tbqCWn2ZuXqa+mptW+A2QcPvIWkD4MkZwgLrxcbqc6L2Y/PqaKm/7VjyO23b3/AzXiArEegPAe83sweBNwLj7j7vcoskohAqUzA7HS+V2noKZmcatpc7nqyj2QtTdybfEDJBc/hYJtnONIRd0DKW3DZbbLldw7xMrmFuQ0DWw7JhvClAc3P3n2kMx5b59drimtwy4AEehnhkeOh45BCBVyM8jOKD4WqERxFeDfFqFapVfHYWr63D+PTfLDldbzx1r5/pLzBmFo/FkxrGG8ZaA2BeICTtJIfmz2+9eUv4BhksMMjEYWOZOIAsk4RoEGBB0DJvbtyCoPm2tT4zvFIhKpfxSgUvl4lKJbxcwSvlpD09t10ux2PlUsN2mahcmtuulPFSPN89wix+bLOkrsDqfQSGBQ31BEnNFkAmaDNvbj6BQRRfAvIogsjxKIz7whD3qGHcIYrmxudtR/P6ht/+o6x95zvP4w9xcUsGupl9Cvh+YL2ZHQF+C8gBuPu9wMPA24ADwDRw14pXeSmIQihPQHkyXkq17Ynm/vpY43hDf3VmmQ9skB+AXD/k+yE3kKz7oG9tsl1b+uLAzObjAM7k4yBru91uTg73gOrkDOH4OcKJc1TPThCeHad65gxeLmN9RYJiH1YsEBT7CPqKWKEYr4t9BMVCvG7oD4pFyOWag6RD7o5PTxNOTRFNTRFNTSfr1mUiXk/H7db5Xi7PhXASxMxeoJ2fvHxBgBWLBPk8VijE/97yhXi7UCDo74t3zLVADUM8rMKsxztobwjeMGofrEvMW2hHYWZx6DfsFCxIjtrbjSc7FAsykIv7gnz+gvzalgx0d79ziXEH3rdiFV1EUaVCeHaMcGyM8PiLhN/+KuGLT+DnzkJYwqrlOICrZYjKQHKwYw2n/rWDsFpW5fog3we5PiwXr8ltifvyfdhQP1YcIOgfwPoGsf5BrH+IoH8IGxjGBlYTDK6CvmEsPwDZwss6RfMoIhwfJzx7lvD0aaqnz1A9M0p4+gzVM6cJzyT9Z84Qnj5NOD7e/o6yWYJ8nqhUiv+xL1cQEBSLWF8fQaEQr4vF+I+2WMRyOaJSqW1Y0+H33gb9/QQDA01LbuNGgoEBrFjAcjksm8Oy2WQ7i+XibbLZeKxtfxbL5Zv6LZuNx3K5+KgU6qf47p5cHfKm/nlj9UsCNI/Nu2TQoKU97zuB5/2qFr99HGCNR6JJqIXxdXoPw5Yj0WRefTxq3+dxMNYDuVAkKCTb+UK840/C2fL5+N9AbayQj3/3smxd+7TFleRRRDQ5GQdzy1JN1tH4eFM7PDuGz3RytBwAfcnScUXEJyvT5/PjJA8bxEci9T+IAlbIx0cpxeLcdsOY5XJEk+cIz9SCOw5swrDtQ2RWryazbh3ZtWspXHMN2bVryaxdS3bdWjJr1zWtg+FhzCwOkNnZOHxLJbxUIpopxafJMyW8NENUKsfrhv6oNIOXyvF6JjmlnpmJ7+fcOaJKhaCvj8zq1eS2bCEYmAvnTGNIDw4S9A/MC+6gvy8+rRZ5Beu5QJ/ev5/T9/9xc3BPTCwYWgQBmeHhOLwGi+TyZYojJTKrxshky2SKkNl0JZmr3kDmulvIXHUjVii2P6KKoob+pM/n+tsdfXkUNfV5NYyvJ7ZeN6w0X19svIZYbyfXDqNKPFadmEzGknnlMsHwMNk1a8ht2ULfa18zL5gza9fGwb1mTXyUuUxmBvk8mXyezPDwef5fFJELoecC3ctlZk+cILN6FYXrro2DetWqeJ0s2dp2X0Bw6pvYC1+C578IY4fiO1mzE678MbjqVrj8+6CoYBKR3tdzgT5w881c8befaT8YhXDsm/D8P8L+L8KRx+OXxOWHYOctcPMvxiG+9oqLW7SIyEXQc4E+z/jR+Oj7+X+BF74MM2cBg82vg+/9lfjF+9tujF/JISKSYr0X6JVpOPjVuRAffS7uH9oE174tDvArfgAG1nW3ThGRi6z3Av2Zz8Lf3QOZAuy4GV7/LrjyVtjwqgv6DiwRkUtd7wX6NT8E7/ob2PHm+DXeIiIC9GKg96+Fq36w21WIiFxy9E4MEZGUUKCLiKSEAl1EJCUU6CIiKaFAFxFJCQW6iEhKKNBFRFJCgS4ikhIKdBGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUqKjQDez28zsW2Z2wMw+2GZ8lZn9vZn9u5k9bWZ3rXypIiKymCUD3cwywEeB24FdwJ1mtqtl2vuAZ9z9BuD7gf9tZvkVrlVERBbRyRH6jcABd3/B3SvAg8DeljkODJmZAYPAGaC6opWKiMiiOgn0LcDhhvaRpK/RHwCvAo4B/wH8krtHrXdkZneb2T4z2zc6OnqeJYuISDudBLq16fOW9g8BTwCbgdcBf2Bmw/Nu5H6fu+92990jIyPLLFVERBbTSaAfAbY1tLcSH4k3ugv4jMcOAN8FrluZEkVEpBOdBPrjwNVmtjN5ovMO4KGWOYeAWwHM7DLgWuCFlSxUREQWl11qgrtXzez9wBeADPAJd3/azO5Jxu8FPgQ8YGb/QXyJ5gPufuoC1i0iIi2WDHQAd38YeLil796G7WPAf17Z0kREZDn0TlERkZRQoIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUkKBLiKSEgp0EZGUUKCLiKSEAl1EJCUU6CIiKaFAFxFJCQW6iEhKKNBFRFJCgS4ikhIKdBGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUqKjQDez28zsW2Z2wMw+uMCc7zezJ8zsaTN7ZGXLFBGRpWSXmmBmGeCjwFuBI8DjZvaQuz/TMGc18DHgNnc/ZGYbLlC9IiKygE6O0G8EDrj7C+5eAR4E9rbMeSfwGXc/BODuJ1e2TBERWUongb4FONzQPpL0NboGWGNmXzaz/Wb20+3uyMzuNrN9ZrZvdHT0/CoWEZG2Ogl0a9PnLe0s8Abgh4EfAn7TzK6ZdyP3+9x9t7vvHhkZWXaxIiKysCWvoRMfkW9raG8FjrWZc8rdp4ApM3sUuAH49opUKSIiS+rkCP1x4Goz22lmeeAO4KGWOZ8Fvs/MsmbWD7wReHZlSxURkcUseYTu7lUzez/wBSADfMLdnzaze5Lxe939WTP7B+BJIALud/enLmThIiLSzNxbL4dfHLt37/Z9+/Z15bFFRHqVme13993txvROURGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUkKBLiKSEgp0EZGUUKCLiKSEAl1EJCUU6CIiKaFAFxFJCQW6iEhKKNBFRFJCgS4ikhIKdBGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSoqNAN7PbzOxbZnbAzD64yLz/ZGahmb1j5UoUEZFOLBnoZpYBPgrcDuwC7jSzXQvM+5/AF1a6SBERWVonR+g3Agfc/QV3rwAPAnvbzPsF4G+AkytYn4iIdKiTQN8CHG5oH0n66sxsC/DjwL2L3ZGZ3W1m+8xs3+jo6HJrFRGRRXQS6Namz1vavw98wN3Dxe7I3e9z993uvntkZKTDEkVEpBPZDuYcAbY1tLcCx1rm7AYeNDOA9cDbzKzq7n+3EkWKiMjSOgn0x4GrzWwncBS4A3hn4wR331nbNrMHgM8pzEVELq4lA93dq2b2fuJXr2SAT7j702Z2TzK+6HVzERG5ODo5QsfdHwYebulrG+Tu/jMvvywREVkuvVNURCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUkKBLiKSEgp0EZGUUKCLiKSEAl1EJCUU6CIiKaFAFxFJCQW6iEhKKNBFRFJCgS4ikhIKdBGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISijQRURSQoEuIpISCnQRkZToKNDN7DYz+5aZHTCzD7YZ/0kzezJZvmpmN6x8qSIispglA93MMsBHgduBXcCdZrarZdp3gT3u/lrgQ8B9K12oiIgsrpMj9BuBA+7+grtXgAeBvY0T3P2r7n42aX4N2LqyZTY7MV66kHcvItKTOgn0LcDhhvaRpG8hPwt8vt2Amd1tZvvMbN/o6GjnVTb4+38/xp7f+RL//MxL53V7EZG06iTQrU2ft51o9gPEgf6BduPufp+773b33SMjI51X2eDNV63n2o1D/Nxf7OezTxw9r/sQEUmjTgL9CLCtob0VONY6ycxeC9wP7HX30ytT3nxrB/J88r1vZPeONfzyXz7Bn3/t4IV6KBGRntJJoD8OXG1mO80sD9wBPNQ4wcy2A58Bfsrdv73yZTYbKub40/fcyFuu3cBv/t1TfPRLBy70Q4qIXPKWDHR3rwLvB74APAt82t2fNrN7zOyeZNr/ANYBHzOzJ8xs3wWrOFHMZbj3p97A3tdt5ne+8C0+/PnncG97JUhE5BUh28kkd38YeLil796G7fcC713Z0paWywT8n594HUPFLPc+8jwTpVk+tPfVZIJ2l/1FRNKto0C/lAWB8aG9r2a4mONjX36eyVKV3/uJG8hl9CZYEXll6flABzAzfv226xjuy/Hhzz/HudIsH/vJN9CXz3S7NBGRiyZVh7H37LmS3/7x1/Dlb4/y7j/5BpOl2W6XJCJy0aQq0AHe+cbtfOSO1/NvB89y5x99jdPnyt0uSUTkokhdoAO8/YbN/NFP7+Y7L53jJ/7wMY6Pz3S7JBGRCy6VgQ7wA9dt4M/ecyMvTZR5x8cf48VTU90uSUTkgkptoAO88Yp1fOq/3sTMbMg77n2MZ49PdLskEZELJtWBDvCarav49M/dRDYw/ssfPsb+g2eXvpGISA9KfaADXLVhiL+6502sHcjzrvu/zle+c6rbJYmIrLieC/Tx8jjPnXmOcri8V69sW9vPp+95EzvW9fOeBx7nH546cYEqFBHpjp57Y9Fjxx/j1x75NQIL2Da0jStWXcFVq6/iytVXcuXqK9m5aieFTKHtbTcMFfnLu9/EzzzwDX7+k/v5X++4gXe84YJ+F4eIyEVj3fpAq927d/u+fcv/DK/R6VH2n9zP82PP15eDEwcJPQToKOinylV+7s/385UDp/itH93FXW/euaI/28VQCSscO3eMo+eO1peT0ye5Zs017Nm2h53DOzHTZ9qIpI2Z7Xf33W3Hei3Q25kNZ3lx4kWeH39+yaC/clUc8DuGdvJXj83ylWcDfuXW6/nFW6+6pAKwGlV5afoljk4ebQrt2jI6PYo3fM9INsiytriWk9MnAdg2tI09W/dwy9Zb2H3ZbnKZXLd+FBFZQakP9IXUg37s+XrYHxg7wKGJQ/WgByMqr2Pb4E5uv+4Grlp9FWuKa8gHeXKZHPkgTz6TJxfk6uvG/sDO72mIyCNOzZyaC+mW4D4xdaKhxniHdFn/ZWwZ3MLmwc1sHdzKlqEtbBmMl5G+ETJBhuPnjvPokUd55MgjfP3416lEFQZyA9y8+Wb2bN3D9275Xtb1rVuB366IdMMrNtAX0hj0B8YO8Llnv8mhc98lWziNEy3rvjKWmQv6JPRb243ralTl6LmjHDt3jEpUabqv9X3rmwN7cEsc2gNb2DiwcdlH2dOz03zjxDd45MgjPHr4UU7OnMQwXjPyGvZs3cOerXu4Zs01XT8zcXdKYYliptj1WkQudQr0Jbg7v//P3+EjX3yWW643fv4tmyGoUgkrzEazzIazzEazVMIKlajCbDhbX9f6Z6O4r/E2TXOTvsACNg9urh9Z18J78+BmitniBf0Znz3zbD3cnzr9FAAbBzbWL83cuPHGC1rDWGmMQ5OHODhxkEOThzg0ES8HJw8yWZmkL9vHpoFNbBrYxMaBjfH24Fx7Y//yd2oiaaNA79Aff+W7fOhzz5DPBlx72RDXbx7m+s3D7No8zKs2DdOf77kXBS3o1Mwp/vXIv/LIkUf46rGvMlOdoZgpctOmm7hl2y3csuUWLhu4bNn3O14ebwrsgxMHOTx5mIMTB5mozL1T1zA2D25m29A2dgzvYEP/Bs6WznJ86jjHp45zYuoEZ0pnmu7bMNb3rZ8X+PXtgU2sLqzWUb6kmgJ9Gf7fgVM88u1Rnj42ztPHJhibjj+C1wx2rh/g+s2r2LVpuB726wbbv0Syl5TDMvtO7IuP3o88ytFzRwF41dpXsWdbfGlm17pd9ecLxsvj8dH15NwRdq09Xh6v369hbBrYxLbhbewY2sH24e3sGN7B9qHtbB3aSj6TX7SuUrXEiakT9YCvhf3xqeO8NPUSx6eOz3s/QjFTbBv2l/VfxkjfCCP9IwznhxX6K6SWH443bddkg/QcBF0qFOjnyd05Nl7imWMT9YB/5tgER8fmPr3xsuEC129eNXc0v2kV29b29WxguDvPjz1fD/cnRp8g8oh1xXVsHtzM4cnDjJXH6vMNY+PARrYPb2f70Fxgbx+OQ3uh9wSsVK1ny/FR/YlzzYFf2wGcmpn/ruB8kGekf4T1fevrIT/SF7c39G+I+/tHWF1Yfd5Penci8ohzs+cYL48zUZ6I15V4PV4Zr7crYYXII0IPqUZVIo+oepUwCuPtqEro57cdRmE9gOtBXF8tHtadyFiGYrZIMVOsrwvZAsVMkb5sH4VMIe5vmdPaV5vXeJusZefV1vgz1Ot3b/uztPvZGn++gIDAAsyseduCuYXmPqNlvKWv9b7OhwJ9hY1NV5KQn+CZ43HYHzh5jij5VQ4Vs8lR/Cp2JUF/1YbBnvxavLHSGF859hUePfwoZ8pn4rBOAnvH8I4LHtovVyWscGLqBCenTzI6M8ro9CinZk7Vt0dn4mWyMjnvttkgOxf6SfA3hX7SN5QfYrIyGYdyEsRN4ZwE9ERloh7c45VxJiuTRL7wk/B92T6G8kMUMgUylomXIDNvOxtkCSwgE2TIWrydDbJkLLPktplR+w+oh0ytXVOb125Ofa419zlOJawwU52hHJYpVUuUwlJ9Xa6WmQlnKFXntsvV8rwXC6TRXa++i199w6+e120V6BdBaTbkuROTTUfzz52YoDQb/8HmMwHXbBzk2suG2bqmjy2r+9i8uo8ta/rYtKpIMaevy+umUrXE6EwS9rWgT9anZk5xcvokp2ZONZ2ddMIwhgvDrMqvYjg/zKrCqrl2sl5ViJfaeG17qUtSaRVGYbwDaAj/UnX+du1lvU07G6x5h1Pf1zTvkJp2Ri07IjOrH9W7OxERkUfxtkdENGzXlpY+p2G7zX3csOEGbtp003n9fhToXRJGzndPnePp2tH8sQm+c3KSk5NlWn/t6wfz9ZDfvLoh8Ff3sXl1kbUD+Z69jJMmlbBSP8I/NX2qfnQ/nB9uCujhwjDD+WGG8kMX9LKNvPIo0C8xlWrESxMljpyd4dhYsozPNLRLzMyGTbcp5oJ6wDcG/+bVRbau7mfjqiL5rIJDJO0WC3Q9Bd0F+WzAtrX9bFvb33bc3RmbnuXo2AxHx+ZCP26XeO65k4xONr+6wwxGBgtcNlxkw1CBkaFCfT0yVGxq6/KOSDop0C9BZsaagTxrBvK8esuqtnPK1ZAT4yWOnq2FfoljYzO8NFnixESJJ4+Oc/pcuf5EbaPhYpYNw0VGBgtsGC7MrYcKbGgI/1V9OV3mEekhCvQeVchm2LFugB3rBhacE0bO6akyo5NlTk6WGZ0oM3quzMmJUrIu88ThMU5OlOdd4oH4idyRoQLrk4Bf25+nv5BhIJ9loJBloJChP59lIJ9pacfbA4Ushez5vzxLRJZHgZ5imcDYMFRkw1CR6xeZ5+6cK1fngr9pXWJ0sszhM9M8eWSM6XLIVKXa9si/ncBgIJ+NdwSFOOz76zuAeGfQ37ADGGgai/sHC1n6C1kGk3a2B1/+KXIxKNAFM2OomGOomOOKkcEl57s75WrEVLnKVBLw05V4e7pS5VyynmpYT5Wrybx4e3SyzIunpuK+Ze4kCtmgfkYwd7aQZTA5QxhsOFsYbNhx6CxC0k6BLstmZhRzGYq5DOuWzv+OuDul2Yhz5Wo9/Gs7gnPlan1HMdUyXps/PjPLsbGZhvGQsMM9RO0sYqCQrV9S6s/PnRnUdwb5TFO7dmbRn8/Sl8vQn4+XYj5Df05nEnLxKdDlkmBm9OUz9OUzjAy9/Hee1s4i6juI2tlCJWS6vpMI62cItZ1GbXyqHHJiolQ/o6jtJJYjlzH6cvHP1J/PUmwM/WS7Nl7bIcT92aY5+WwQL5m5da7WTvpyGdMORBTokk6NZxHrV+gD1KLIKVXDtpeQZiohM7Mh05WQUrKemQ2ZqcQ7kpnZiJlKlZnZsP58RW18phIyPdv5GcVCAiMJ94BCsq7vADLNO4VcxshnA7JB0PBuymRdf6dlrd1+vLGv8R2ZtdsY1Hc8hdadUkt7bnz+DqzQOJ70N+684ndfxi8CiDxe4u34/1mY9EUR8XbUMqe+PTfH3ckGAZnAyGaMbGBxO9nOBEaupZ0NrOuX7joKdDO7DfgIkAHud/cPt4xbMv42YBr4GXf/txWuVaSrgsCSo+fsipxFtKpUo4YdQ7Ue+JVqRDmMmK1GVMKI2TCiUk2W0KlU5/pmw4hybV7r/NCpVOP7nk1uV43ij6ao70qaP5+r4YOrau25euc+/Gr+GMRBWa8nqWWl3scYGARmSfiuzH2uhMAgmwmaQj4TxDvQubZx543bee/3XbHij79koJtZBvgo8FbgCPC4mT3k7s80TLsduDpZ3gh8PFmLSIdqR6CrSOeXeLg71cgbdkbxutzSjrfD+lh9vGWO4wRmBBaHZGDxTjeT9AVJXzxWm0fzbWpz6vPjOYYRRk41iqhG8RF8NUzWtf6GdhhFzLa0a7eL+xvuJ/IVO2ts1ckR+o3AAXd/AcDMHgT2Ao2Bvhf4M493518zs9Vmtsndj694xSLSk8yMXMbIZQIGLt0P6OxpnTyLsgU43NA+kvQtdw5mdreZ7TOzfaOjo8utVUREFtFJoLe7yt961aqTObj7fe6+2913j4yMdFKfiIh0qJNAPwJsa2hvBY6dxxwREbmAOgn0x4GrzWynmeWBO4CHWuY8BPy0xW4CxnX9XETk4lrySVF3r5rZ+4EvEL9s8RPu/rSZ3ZOM3ws8TPySxQPEL1u868KVLCIi7XT0OnR3f5g4tBv77m3YduB9K1uaiIgsh94rLCKSEgp0EZGU6Np3iprZKHDwPG++Hji1guVcTKq9O1R7d/Rq7Zdy3Tvcve3rvrsW6C+Hme1b6EtSL3WqvTtUe3f0au29WrcuuYiIpIQCXUQkJXo10O/rdgEvg2rvDtXeHb1ae0/W3ZPX0EVEZL5ePUIXEZEWCnQRkZTouUA3s9vM7FtmdsDMPtjtejplZtvM7Etm9qyZPW1mv9TtmpbDzDJm9k0z+1y3a1mO5MtW/trMnkt+92/qdk2dMrNfSf6tPGVmnzKzYrdrWoiZfcLMTprZUw19a83sn8zsO8l6TTdrXMgCtf9O8m/mSTP7WzNb3cUSO9ZTgd7wdXi3A7uAO81sV3er6lgV+G/u/irgJuB9PVQ7wC8Bz3a7iPPwEeAf3P064AZ65Gcwsy3ALwK73f3VxB+Md0d3q1rUA8BtLX0fBL7o7lcDX0zal6IHmF/7PwGvdvfXAt8GfuNiF3U+eirQafg6PHevALWvw7vkufvx2hdnu/skcbDM+1anS5GZbQV+GLi/27Ush5kNA7cAfwzg7hV3H+tqUcuTBfrMLAv0cwl/x4C7PwqcaeneC/xpsv2nwI9dzJo61a52d/9Hd68mza8Rf8fDJa/XAr2jr7q71JnZ5cDrga93uZRO/T7w60DU5TqW6wpgFPiT5HLR/WY20O2iOuHuR4HfBQ4Bx4m/Y+Afu1vVsl1W+16EZL2hy/Wcr/cAn+92EZ3otUDv6KvuLmVmNgj8DfDL7j7R7XqWYmY/Apx09/3druU8ZIHvAT7u7q8Hprh0T/ubJNeb9wI7gc3AgJm9q7tVvfKY2X8nvlz6yW7X0oleC/Se/qo7M8sRh/kn3f0z3a6nQ28G3m5mLxJf4nqLmf1Fd0vq2BHgiLvXzoT+mjjge8EPAt9191F3nwU+A9zc5ZqW6yUz2wSQrE92uZ5lMbN3Az8C/KT3yBt2ei3QO/k6vEuSmRnxtdxn3f33ul1Pp9z9N9x9q7tfTvz7/hd374kjRXc/ARw2s2uTrluBZ7pY0nIcAm4ys/7k386t9MgTug0eAt6dbL8b+GwXa1kWM7sN+ADwdnef7nY9neqpQE+epKh9Hd6zwKfd/enuVtWxNwM/RXyE+0SyvK3bRb0C/ALwSTN7Engd8NvdLaczyVnFXwP/BvwH8d/qJft2dDP7FPAYcK2ZHTGznwU+DLzVzL4DvDVpX3IWqP0PgCHgn5K/1XsXvZNLhN76LyKSEj11hC4iIgtToIuIpIQCXUQkJRToIiIpoUAXEUkJBbqISEoo0EVEUuL/A9Nf9lbPK8HSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313acb7a",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c94e929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 80,394\n",
      "Trainable params: 80,010\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full = create_model_struc()\n",
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "570eae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b5a94b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ea996d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "   2/2625 [..............................] - ETA: 10:23 - loss: 3.3984 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.4661s). Check your callbacks.\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.1792 - accuracy: 0.9453\n",
      "Epoch 2/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0815 - accuracy: 0.9746\n",
      "Epoch 3/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0613 - accuracy: 0.9808\n",
      "Epoch 4/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0483 - accuracy: 0.9843\n",
      "Epoch 5/14\n",
      "2625/2625 [==============================] - 26s 10ms/step - loss: 0.0395 - accuracy: 0.9873\n",
      "Epoch 6/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0337 - accuracy: 0.9889\n",
      "Epoch 7/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0280 - accuracy: 0.9910\n",
      "Epoch 8/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0234 - accuracy: 0.9921\n",
      "Epoch 9/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0210 - accuracy: 0.9930\n",
      "Epoch 10/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0173 - accuracy: 0.9941\n",
      "Epoch 11/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0152 - accuracy: 0.9949\n",
      "Epoch 12/14\n",
      "2625/2625 [==============================] - 27s 10ms/step - loss: 0.0140 - accuracy: 0.9955\n",
      "Epoch 13/14\n",
      "2625/2625 [==============================] - 28s 11ms/step - loss: 0.0127 - accuracy: 0.9957\n",
      "Epoch 14/14\n",
      "2625/2625 [==============================] - 28s 11ms/step - loss: 0.0112 - accuracy: 0.9962\n"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=50, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bbcd4240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90631a51",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d76a9",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4a71a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c821a72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7517b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3806",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bdbf6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5ead1",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0f1c0995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "201a9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [3.6760672e-19 2.7813120e-22 1.0000000e+00 4.9501728e-15 1.5551702e-16\n",
      " 4.5903582e-19 1.1105674e-20 1.1551887e-16 6.7895241e-18 7.3449477e-19]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fc717f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "188b2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "151156ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f274414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7408b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae09ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
