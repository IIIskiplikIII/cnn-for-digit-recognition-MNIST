{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ee88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork of my previous developed notebook for digit recognition. Therefore you will find some parts that look common the the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> and some parts that are completely different.\n",
    "\n",
    "With this I want to take a deeper look in some parts of finetuning hyperparameters. The following list shows some of the finetuning parameters which I will take a look into, one or two ore more ... :\n",
    "- Dwindling / Exploding Gradients\n",
    "    - <b>Initializing the Weights</b>\n",
    "    - <b>Batchnormalization</b>\n",
    "    - <s>Gradient Clipping</s>\n",
    "    - <b>Saturated Activataion Functions</b>\n",
    "- Optimizers\n",
    "    - <s>Momentum Optimizers</s>\n",
    "    - <s>Nesterov</s>\n",
    "    - <s>AdaGrad</s>\n",
    "    - <s>RMSProp</s>\n",
    "    - <b>Adam - Optimizer</b>\n",
    "    - <s>Scheduling Learnrate</s>\n",
    "- Regulations\n",
    "    - <s>Drop-Outs</s>\n",
    "    - <b>l1 / l2 - Regulations</b>\n",
    "    - <s>Monte-Carlo Drop-out ???</s>\n",
    "    - <s>Max Norm Regulations ????</s>\n",
    "\n",
    "Not part of this notebook will be the use of pretrained neural networks (Transferlearning). I just want to list this here for the sake of completeness.\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "## Notebook Versions with Different Hyperparameter Configurations\n",
    "As described in the part above, I used/tested different hyperparameter settings to get a little bit closer to its effects on the neural network and the network's results. I know that there are parameters that effect other parameters when they have changed (and therefore should have been changed as well), however in these cases I just tried a little bit around. Sometimes I kept one or two parameters together, which should be together (e.g. kernel initializer \"lecun\" and activation function \"selu\") and sometimes not. The main purpose here was to use them and see the results.\n",
    "\n",
    "Therefore on Kaggle you can look in the different versions of this notebook if you are interested. In the following I will list some versions with the used hyperparameter config in it:\n",
    "- Version 7 and 6:\n",
    "    - Activation Function - \"relu\"\n",
    "    - Initializing Weights - \"He Normalization\"\n",
    "    - Batchnormalization\n",
    "- Version 9:\n",
    "    - Activation Function - \"selu\"\n",
    "    - Initializing Weights - \"LeCun Normal\"\n",
    "- Version 12 and 14:\n",
    "    - Regularisation with L1 and L2\n",
    "- Version 15:\n",
    "    - Activation Function - \"relu\"\n",
    "    - Initializing Weights - \"He Normalization\"\n",
    "    - Batchnormalization\n",
    "    - Optimizer - \"Adam\"\n",
    "\n",
    "The current best run was based on the Version 7 with an accuracy of 0.97657 on the kaggle competition \"Digit Recognzier\"\n",
    "\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Digit Recognition with a Deep Neural Network: https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "\n",
    "    ## final solution\n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x000001E968C91820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x000001E968C91820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_11_12-08_56_46\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_11_12-08_56_46'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "236d0a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_all.element_spec[0]\n",
    "##val_dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28,1]\n",
    "\n",
    "batch_shape = []\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None\n",
    "\n",
    "# Model building\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation_fn))\n",
    "#max pooling\n",
    "model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation_fn))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 137,994\n",
      "Trainable params: 137,610\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   1/2100 [..............................] - ETA: 0s - loss: 3.0415 - accuracy: 0.0625WARNING:tensorflow:From D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_train_batch_end` time: 0.0265s). Check your callbacks.\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.2307 - accuracy: 0.9321 - val_loss: 0.1346 - val_accuracy: 0.9602\n",
      "Epoch 2/200\n",
      "2100/2100 [==============================] - 24s 12ms/step - loss: 0.1035 - accuracy: 0.9676 - val_loss: 0.1009 - val_accuracy: 0.9684\n",
      "Epoch 3/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0785 - accuracy: 0.9759 - val_loss: 0.0886 - val_accuracy: 0.9721\n",
      "Epoch 4/200\n",
      "2100/2100 [==============================] - 24s 12ms/step - loss: 0.0670 - accuracy: 0.9799 - val_loss: 0.0829 - val_accuracy: 0.9751\n",
      "Epoch 5/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0742 - val_accuracy: 0.9780\n",
      "Epoch 6/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0505 - accuracy: 0.9849 - val_loss: 0.0617 - val_accuracy: 0.9815\n",
      "Epoch 7/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0461 - accuracy: 0.9860 - val_loss: 0.0739 - val_accuracy: 0.9784\n",
      "Epoch 8/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0425 - accuracy: 0.9875 - val_loss: 0.0596 - val_accuracy: 0.9823\n",
      "Epoch 9/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0370 - accuracy: 0.9891 - val_loss: 0.0654 - val_accuracy: 0.9804\n",
      "Epoch 10/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0343 - accuracy: 0.9900 - val_loss: 0.0499 - val_accuracy: 0.9848\n",
      "Epoch 11/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.0537 - val_accuracy: 0.9840\n",
      "Epoch 12/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0323 - accuracy: 0.9899 - val_loss: 0.0636 - val_accuracy: 0.9807\n",
      "Epoch 13/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 0.0504 - val_accuracy: 0.9854\n",
      "Epoch 14/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0296 - accuracy: 0.9914 - val_loss: 0.0558 - val_accuracy: 0.9842\n",
      "Epoch 15/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0269 - accuracy: 0.9921 - val_loss: 0.0525 - val_accuracy: 0.9854\n",
      "Epoch 16/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.0594 - val_accuracy: 0.9835\n",
      "Epoch 17/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0534 - val_accuracy: 0.9851\n",
      "Epoch 18/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0253 - accuracy: 0.9925 - val_loss: 0.0437 - val_accuracy: 0.9877\n",
      "Epoch 19/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0247 - accuracy: 0.9928 - val_loss: 0.0434 - val_accuracy: 0.9874\n",
      "Epoch 20/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0487 - val_accuracy: 0.9861\n",
      "Epoch 21/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.0612 - val_accuracy: 0.9843\n",
      "Epoch 22/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.0498 - val_accuracy: 0.9858\n",
      "Epoch 23/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.0471 - val_accuracy: 0.9858\n",
      "Epoch 24/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0473 - val_accuracy: 0.9875\n",
      "Epoch 25/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.0522 - val_accuracy: 0.9842\n",
      "Epoch 26/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0222 - accuracy: 0.9935 - val_loss: 0.0479 - val_accuracy: 0.9874\n",
      "Epoch 27/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0538 - val_accuracy: 0.9850\n",
      "Epoch 28/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.0458 - val_accuracy: 0.9873\n",
      "Epoch 29/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.0467 - val_accuracy: 0.9868\n",
      "Epoch 30/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 0.0488 - val_accuracy: 0.9871\n",
      "Epoch 31/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0197 - accuracy: 0.9941 - val_loss: 0.0468 - val_accuracy: 0.9865\n",
      "Epoch 32/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0492 - val_accuracy: 0.9871\n",
      "Epoch 33/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0469 - val_accuracy: 0.9870\n",
      "Epoch 34/200\n",
      "2100/2100 [==============================] - 26s 12ms/step - loss: 0.0177 - accuracy: 0.9947 - val_loss: 0.0494 - val_accuracy: 0.9865\n",
      "Epoch 35/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.0502 - val_accuracy: 0.9867\n",
      "Epoch 36/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.0490 - val_accuracy: 0.9869\n",
      "Epoch 37/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.0584 - val_accuracy: 0.9852\n",
      "Epoch 38/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 0.0512 - val_accuracy: 0.9875\n",
      "Epoch 39/200\n",
      "2100/2100 [==============================] - 25s 12ms/step - loss: 0.0173 - accuracy: 0.9948 - val_loss: 0.0486 - val_accuracy: 0.9874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=200, validation_data=val_ds, callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=20), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model building\n",
    "model_full = keras.models.Sequential()\n",
    "\n",
    "model_full.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Activation(activation_fn))\n",
    "model_full.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Activation(activation_fn))\n",
    "model_full.add(keras.layers.Flatten())\n",
    "model_full.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "model_full.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 137,994\n",
      "Trainable params: 137,610\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "   2/2625 [..............................] - ETA: 9:06 - loss: 2.7900 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.4050s). Check your callbacks.\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.2074 - accuracy: 0.9384\n",
      "Epoch 2/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0963 - accuracy: 0.9709\n",
      "Epoch 3/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0732 - accuracy: 0.9776\n",
      "Epoch 4/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0582 - accuracy: 0.9823\n",
      "Epoch 5/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0501 - accuracy: 0.9847\n",
      "Epoch 6/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0453 - accuracy: 0.9859\n",
      "Epoch 7/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0398 - accuracy: 0.9875\n",
      "Epoch 8/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0369 - accuracy: 0.9887\n",
      "Epoch 9/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0345 - accuracy: 0.9896\n",
      "Epoch 10/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0323 - accuracy: 0.9900\n",
      "Epoch 11/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0309 - accuracy: 0.9904\n",
      "Epoch 12/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0293 - accuracy: 0.9912\n",
      "Epoch 13/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0284 - accuracy: 0.9914\n",
      "Epoch 14/60\n",
      "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0262 - accuracy: 0.9921\n",
      "Epoch 15/60\n",
      "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0258 - accuracy: 0.9920\n",
      "Epoch 16/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0257 - accuracy: 0.9922\n",
      "Epoch 17/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0228 - accuracy: 0.9932\n",
      "Epoch 18/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0244 - accuracy: 0.9926\n",
      "Epoch 19/60\n",
      "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0228 - accuracy: 0.9933\n",
      "Epoch 20/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0227 - accuracy: 0.9929\n",
      "Epoch 21/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0222 - accuracy: 0.9935\n",
      "Epoch 22/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0205 - accuracy: 0.9937\n",
      "Epoch 23/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0205 - accuracy: 0.9939\n",
      "Epoch 24/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0197 - accuracy: 0.9941\n",
      "Epoch 25/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0208 - accuracy: 0.9939\n",
      "Epoch 26/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0190 - accuracy: 0.9944\n",
      "Epoch 27/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0196 - accuracy: 0.9940\n",
      "Epoch 28/60\n",
      "2625/2625 [==============================] - 29s 11ms/step - loss: 0.0194 - accuracy: 0.9939\n",
      "Epoch 29/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0179 - accuracy: 0.9945\n",
      "Epoch 30/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0187 - accuracy: 0.9943\n",
      "Epoch 31/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0180 - accuracy: 0.9944\n",
      "Epoch 32/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0177 - accuracy: 0.9945\n",
      "Epoch 33/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0190 - accuracy: 0.9944\n",
      "Epoch 34/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0168 - accuracy: 0.9948\n",
      "Epoch 35/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0170 - accuracy: 0.9952\n",
      "Epoch 36/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0179 - accuracy: 0.9948\n",
      "Epoch 37/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0164 - accuracy: 0.9951\n",
      "Epoch 38/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0158 - accuracy: 0.9951\n",
      "Epoch 39/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0162 - accuracy: 0.9949\n",
      "Epoch 40/60\n",
      "2625/2625 [==============================] - 31s 12ms/step - loss: 0.0159 - accuracy: 0.9951\n",
      "Epoch 41/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0177 - accuracy: 0.9947\n",
      "Epoch 42/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0156 - accuracy: 0.9955\n",
      "Epoch 43/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0154 - accuracy: 0.9954\n",
      "Epoch 44/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0149 - accuracy: 0.9955\n",
      "Epoch 45/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0152 - accuracy: 0.9954\n",
      "Epoch 46/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0149 - accuracy: 0.9957\n",
      "Epoch 47/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 48/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0147 - accuracy: 0.9954\n",
      "Epoch 49/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0146 - accuracy: 0.9954\n",
      "Epoch 50/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 51/60\n",
      "2625/2625 [==============================] - 30s 12ms/step - loss: 0.0141 - accuracy: 0.9957\n",
      "Epoch 52/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0132 - accuracy: 0.9960\n",
      "Epoch 53/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0144 - accuracy: 0.9954\n",
      "Epoch 54/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 55/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0149 - accuracy: 0.9954\n",
      "Epoch 56/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 57/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0136 - accuracy: 0.9958\n",
      "Epoch 58/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0125 - accuracy: 0.9958\n",
      "Epoch 59/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0128 - accuracy: 0.9963\n",
      "Epoch 60/60\n",
      "2625/2625 [==============================] - 30s 11ms/step - loss: 0.0136 - accuracy: 0.9958\n"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=60, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [9.5746900e-18 1.6445102e-18 1.0000000e+00 7.1260437e-12 2.3395641e-14\n",
      " 6.0765847e-16 4.1415867e-20 6.7374364e-17 5.5961521e-17 5.5111600e-16]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
