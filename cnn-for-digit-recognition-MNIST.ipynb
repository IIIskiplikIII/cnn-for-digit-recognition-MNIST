{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d55c3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03773a7f",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork or copy of my previous developed notebook for digit recognition. Therefore you will find some parts that look common to the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> or <a href=\"https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\">Finetuning Hyperparameters in Deep Neural Network</a>.\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "My focus on this notebook lies in using Convolutional Neural Networks. I worked with them before but since I read different articles and books about its architecture, I got a deeper understanding of the different layers and their result to the rest of the network. \n",
    "\n",
    "As in the previous notebooks I will use different architecture / layer configurations and submit the results to the Kaggle competition to get a rated accuracy value. This will be an indicator for the used model architecture. The plan is to commit the notebook in Git as well as in Kaggle to versionize the architecture with its accuracy value. This will help me to understand the benefits of the different layers a little and look into the progress of the different architectures later.\n",
    "\n",
    "The idea is to use different layers in different combinations. The following layers will be used in this notebook:\n",
    "\n",
    "- Convolutional layers (Conv2D, Conv3D,...)\n",
    "- Max Pooling layers\n",
    "- Avg Pooling  \n",
    "- Batch Normalization\n",
    "- Dropout\n",
    "\n",
    "\n",
    "Not part of this notebook will be the architecture of Transfer-Learning where a pretrained model is used and retrained with a new dataset. I already tried that approach in the following notebook: https://www.kaggle.com/skiplik/picturerecognition-tf-and-transferlearning-resnet\n",
    "\n",
    "## Best Runs\n",
    "The best run was based on Kaggle version 6 with an accuracy of 99.12% on the kaggle competition \"Digit Recognizer\". For this version the special improvement was (next to the Conv2D layers, the reduction of the epochs for training): https://www.kaggle.com/skiplik/cnn-for-digit-recognition-mnist?scriptVersionId=79696075\n",
    "\n",
    "Right after that there where an equivalent run with an accuracy of 99.025%. Here happened some architecture changes, added more filters to the Conv-Layers and increased the kernel size. Due to the observation of the training process (via tensorboard) I recognized the validation accuracy starts to get a little bit noisy, therefore I decided to reduce the epochs number to a value where the accuracy still got its none noisy behaviour. In my opinion this is the most important configuration for this accuracy value. The reduction of the epochs just saves the model's capability of giving better answers to totally new, unseen data.\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Finetuning Hyperparameters in Deep Neural Network:\n",
    "    - https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\n",
    "- Digit Recognition with a Deep Neural Network: \n",
    "    - https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try:\n",
    "    - https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters:\n",
    "    - https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b10659",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ee3cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6be608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e48d5",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4345f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998993c",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3873e566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22da4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd32c7",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dba0f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7160ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0ad22",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8f365352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeef7ff",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988c85b",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a83d5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "\n",
    "    ## final solution\n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "899fafd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cab30ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c8f5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a12f699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x00000242D78BEB80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x00000242D78BEB80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ed4b5519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "caa4ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51b2e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e8e4e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9edabf",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c859486",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7332d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d2d5fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_11_25-16_28_28\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c880e991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_11_25-16_28_28'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11325f2",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b49fca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdccd2e6",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cac61847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28,1]\n",
    "\n",
    "batch_shape = []\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None\n",
    "\n",
    "# Model building\n",
    "def create_model_struc():  \n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=6, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "    model.add(keras.layers.MaxPooling2D(2))\n",
    "    ## \n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(activation_fn))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n",
    "    model.build()\n",
    "\n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed905797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 256)       9472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 455,434\n",
      "Trainable params: 454,410\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_struc()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674dcbb",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5fceea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bdf28f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4fd797c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/2100 [..............................] - ETA: 9:02 - loss: 2.4520 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0275s vs `on_train_batch_end` time: 0.4900s). Check your callbacks.\n",
      "2100/2100 [==============================] - 64s 31ms/step - loss: 0.1348 - accuracy: 0.9603 - val_loss: 0.1082 - val_accuracy: 0.9673\n",
      "Epoch 2/10\n",
      "2100/2100 [==============================] - 65s 31ms/step - loss: 0.0609 - accuracy: 0.9810 - val_loss: 0.0763 - val_accuracy: 0.9756\n",
      "Epoch 3/10\n",
      "2100/2100 [==============================] - 66s 31ms/step - loss: 0.0448 - accuracy: 0.9853 - val_loss: 0.0597 - val_accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "2100/2100 [==============================] - 65s 31ms/step - loss: 0.0380 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9817\n",
      "Epoch 5/10\n",
      "2100/2100 [==============================] - 64s 30ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.0531 - val_accuracy: 0.9836\n",
      "Epoch 6/10\n",
      "2100/2100 [==============================] - 64s 30ms/step - loss: 0.0237 - accuracy: 0.9924 - val_loss: 0.0654 - val_accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "2100/2100 [==============================] - 63s 30ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.0480 - val_accuracy: 0.9854\n",
      "Epoch 8/10\n",
      "2100/2100 [==============================] - 63s 30ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.0410 - val_accuracy: 0.9883\n",
      "Epoch 9/10\n",
      "2100/2100 [==============================] - 64s 30ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.0440 - val_accuracy: 0.9868\n",
      "Epoch 10/10\n",
      "2100/2100 [==============================] - 64s 31ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=8), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf39b12",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c5628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313acb7a",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 137,994\n",
      "Trainable params: 137,610\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full = create_model_struc()\n",
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570eae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a94b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea996d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24692/4069569132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_ds_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_cb_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_cb_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_full' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=11, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbcd4240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90631a51",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d76a9",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a71a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c821a72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7517b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3806",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdbf6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5ead1",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f1c0995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "201a9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [1.0828646e-13 1.4674600e-16 1.0000000e+00 2.5054231e-10 3.0301879e-09\n",
      " 1.2468671e-12 9.9274750e-17 1.4983346e-11 3.9261570e-14 2.2130670e-13]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc717f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "188b2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151156ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f274414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7408b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae09ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
