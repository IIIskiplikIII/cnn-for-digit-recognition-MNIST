{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f68fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.3.0\n",
      "Keras Version:  2.4.0\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\keras_reg_jl_160_10_002.sav\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\sample_submission.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\test.csv\n",
      "../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "#import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import seed\n",
    "seed(1)\n",
    "seed = 43\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import image\n",
    "from tensorflow import core\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Keras Version: \",keras.__version__)\n",
    "\n",
    "\n",
    "kaggle = 0 # Kaggle path active = 1\n",
    "\n",
    "# change your local path here\n",
    "if kaggle == 1 :\n",
    "    MNIST_PATH= '../input/digit-recognizer'\n",
    "else:\n",
    "    MNIST_PATH= '../Digit_Recognition_with_a_Deep_Neural_Network/data/input/digit-recognizer'\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(MNIST_PATH): \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95d62b",
   "metadata": {},
   "source": [
    "# Introduction - MNIST Training Competition\n",
    "This notebook is a fork or copy of my previous developed notebook for digit recognition. Therefore you will find some parts that look common to the notebook <a href=\"https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\">Digit Recognition with a Deep Neural Network</a> or <a href=\"https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\">Finetuning Hyperparameters in Deep Neural Network</a>.\n",
    "\n",
    "\n",
    "Link to the data topic: https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "As in the previous notebooks I will use Tensorflow with Keras. I already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.\n",
    "\n",
    "The current best run was based on the Version 7 with an accuracy of 0.97657 on the kaggle competition \"Digit Recognzier\"\n",
    "\n",
    "\n",
    "## My other Projects\n",
    "If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n",
    "- Finetuning Hyperparameters in Deep Neural Network:\n",
    "    - https://www.kaggle.com/skiplik/finetuning-hyperparameters-in-deep-neural-network\n",
    "- Digit Recognition with a Deep Neural Network: \n",
    "    - https://www.kaggle.com/skiplik/digit-recognition-with-a-deep-neural-network\n",
    "- Another MNIST Try:\n",
    "    - https://www.kaggle.com/skiplik/another-mnist-try\n",
    "- First NN by Detecting Handwritten Characters:\n",
    "    - https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3c488",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea70e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path and file\n",
    "CSV_FILE_TRAIN='train.csv'\n",
    "CSV_FILE_TEST='test.csv'\n",
    "\n",
    "def load_mnist_data(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def load_mnist_data_manuel(minist_path, csv_file):\n",
    "    csv_path = os.path.join(minist_path, csv_file)\n",
    "    csv_file = open(csv_path, 'r')\n",
    "    csv_data = csv_file.readlines()\n",
    "    csv_file.close()\n",
    "    return csv_data\n",
    "\n",
    "def split_train_val(data, val_ratio):\n",
    "    return \n",
    "    \n",
    "\n",
    "train = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\n",
    "test = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da53d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label'].copy()\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "# competition dataset\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eeab4c",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cb4681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features:  (42000, 784)\n",
      "Shape of the Labels:  (42000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Features: \",X.shape)\n",
    "print(\"Shape of the Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdb9d9",
   "metadata": {},
   "source": [
    "### Label Value Count\n",
    "Visualizing the label distribution of the full train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28fe0d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e39324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.20\n",
    "                                                  , stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40013c9",
   "metadata": {},
   "source": [
    "Comparing the equally splitted train- and val-sets based on the given label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56586361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Set Distribution\n",
      "1    0.111518\n",
      "7    0.104792\n",
      "3    0.103601\n",
      "9    0.099702\n",
      "2    0.099464\n",
      "6    0.098512\n",
      "0    0.098363\n",
      "4    0.096964\n",
      "8    0.096726\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "Val - Set Distribution\n",
      "1    0.111548\n",
      "7    0.104762\n",
      "3    0.103571\n",
      "9    0.099762\n",
      "2    0.099405\n",
      "0    0.098452\n",
      "6    0.098452\n",
      "4    0.096905\n",
      "8    0.096786\n",
      "5    0.090357\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train - Set Distribution\")\n",
    "print(y_train.value_counts() / y_train.value_counts().sum() )\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Val - Set Distribution\")\n",
    "print(y_val.value_counts() / y_val.value_counts().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba845a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (42000, 784)\n",
      "X_train:  (33600, 784)\n",
      "X_val:  (8400, 784)\n",
      "y_train:  (33600,)\n",
      "y_val:  (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X: \", X.shape)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df5184",
   "metadata": {},
   "source": [
    "## Building Transforming Piplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61abc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('normalizer', Normalizer())\n",
    "    ('std_scalar',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb8774",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe842d",
   "metadata": {},
   "source": [
    "### Data Augmentation with Tensorflow Data Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f48a8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 85 // 100       # croping to 90% of the initial picture \n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 1])\n",
    "\n",
    "\n",
    "def crop_flip_resize(image, label, flipping = True):\n",
    "    if flipping == True:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = random_crop(image)\n",
    "    \n",
    "    resized_image = tf.image.resize(cropped_image, [28,28])\n",
    "    final_image = resized_image\n",
    "    #final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8339656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc742591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train_crop = X_train.copy()\n",
    "X_val_crop = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "978acd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensorbased dataset \n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "training_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_train_crop, tf.float32),\n",
    "            tf.cast(y_train, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_crop_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "             tf.cast(X_val_crop, tf.float32),\n",
    "             tf.cast(y_val, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89cf215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function random_crop at 0x0000017B9B99C820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function random_crop at 0x0000017B9B99C820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# resizing, croping images via self build function\n",
    "training_crop_dataset = training_crop_dataset.map(partial(crop_flip_resize, flipping=False))\n",
    "val_crop_dataset = val_crop_dataset.map(partial(crop_flip_resize, flipping=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "569e6db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing a croped, flipped, resized image from new dataset.\n",
    "for X_values, y_values in training_crop_dataset.take(1):\n",
    "    for index in range(1):\n",
    "        plt.imshow(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d7adc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the two datasets\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3baa029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_all length:  67200\n",
      "val_dataset_all length:  16800\n"
     ]
    }
   ],
   "source": [
    "print(\"training_dataset_all length: \", len(list(training_dataset_all)))\n",
    "print(\"val_dataset_all length: \", len(list(val_dataset_all)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59d0a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling and batching data\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "train_ds = training_dataset_all.shuffle(10000).batch(32).prefetch(1)\n",
    "val_ds = val_dataset_all.shuffle(8000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117edd9",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d7f55",
   "metadata": {},
   "source": [
    "## Preparing Model Visualization with Tensorboard (not for Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e88b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative root_logdir:  ../../tensorboard-logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = \"../../tensorboard-logs\"\n",
    "\n",
    "print(\"Relative root_logdir: \",root_logdir)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3395730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run logdir for Tensorboard:  ../../tensorboard-logs\\run_2021_11_14-14_08_31\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "print(\"Current run logdir for Tensorboard: \", run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6c72ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../tensorboard-logs\\\\run_2021_11_14-14_08_31'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f43bd",
   "metadata": {},
   "source": [
    "### Keras Callbacks for Tensorboard\n",
    "With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "369a4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab1a9e",
   "metadata": {},
   "source": [
    "## Building Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "236d0a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_all.element_spec[0]\n",
    "##val_dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc6931a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "\n",
    "input_shape=[784]\n",
    "input_shape_notFlattened=[28,28,1]\n",
    "\n",
    "batch_shape = []\n",
    "\n",
    "\n",
    "learning_rt = 1e-03 \n",
    "activation_fn = \"relu\"\n",
    "initializer = \"he_normal\"\n",
    "regularizer =  None\n",
    "\n",
    "# Model building\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation_fn))\n",
    "model.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation_fn))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f9d7661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 137,994\n",
      "Trainable params: 137,610\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686c4d0",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d46fa028",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_train_model.h5\", save_best_only=True, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806d555",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "107aa699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "   1/2100 [..............................] - ETA: 0s - loss: 3.0415 - accuracy: 0.0625WARNING:tensorflow:From D:\\anaconda3\\envs\\wingpuflake_keras\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0295s). Check your callbacks.\n",
      "2100/2100 [==============================] - 27s 13ms/step - loss: 0.2293 - accuracy: 0.9321 - val_loss: 0.1444 - val_accuracy: 0.9557\n",
      "Epoch 2/65\n",
      "2100/2100 [==============================] - 28s 14ms/step - loss: 0.1027 - accuracy: 0.9679 - val_loss: 0.0899 - val_accuracy: 0.9728\n",
      "Epoch 3/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0795 - accuracy: 0.9758 - val_loss: 0.0955 - val_accuracy: 0.9697\n",
      "Epoch 4/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0654 - accuracy: 0.9807 - val_loss: 0.0748 - val_accuracy: 0.9764\n",
      "Epoch 5/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0573 - accuracy: 0.9826 - val_loss: 0.0943 - val_accuracy: 0.9722\n",
      "Epoch 6/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0493 - accuracy: 0.9852 - val_loss: 0.0675 - val_accuracy: 0.9787\n",
      "Epoch 7/65\n",
      "2100/2100 [==============================] - 28s 13ms/step - loss: 0.0451 - accuracy: 0.9860 - val_loss: 0.0730 - val_accuracy: 0.9792\n",
      "Epoch 8/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0558 - val_accuracy: 0.9834\n",
      "Epoch 9/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0377 - accuracy: 0.9883 - val_loss: 0.0523 - val_accuracy: 0.9840\n",
      "Epoch 10/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.0588 - val_accuracy: 0.9833\n",
      "Epoch 11/65\n",
      "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 0.0545 - val_accuracy: 0.9845\n",
      "Epoch 12/65\n",
      "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0306 - accuracy: 0.9904 - val_loss: 0.0526 - val_accuracy: 0.9841\n",
      "Epoch 13/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0309 - accuracy: 0.9907 - val_loss: 0.0575 - val_accuracy: 0.9823\n",
      "Epoch 14/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0284 - accuracy: 0.9911 - val_loss: 0.0520 - val_accuracy: 0.9849\n",
      "Epoch 15/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0569 - val_accuracy: 0.9837\n",
      "Epoch 16/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.0539 - val_accuracy: 0.9834\n",
      "Epoch 17/65\n",
      "2100/2100 [==============================] - 30s 14ms/step - loss: 0.0269 - accuracy: 0.9919 - val_loss: 0.0465 - val_accuracy: 0.9864\n",
      "Epoch 18/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0521 - val_accuracy: 0.9850\n",
      "Epoch 19/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0609 - val_accuracy: 0.9828\n",
      "Epoch 20/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0252 - accuracy: 0.9924 - val_loss: 0.0549 - val_accuracy: 0.9851\n",
      "Epoch 21/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0532 - val_accuracy: 0.9837\n",
      "Epoch 22/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0236 - accuracy: 0.9931 - val_loss: 0.0552 - val_accuracy: 0.9849\n",
      "Epoch 23/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0239 - accuracy: 0.9929 - val_loss: 0.0467 - val_accuracy: 0.9861\n",
      "Epoch 24/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.0424 - val_accuracy: 0.9870\n",
      "Epoch 25/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.0477 - val_accuracy: 0.9871\n",
      "Epoch 26/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.0527 - val_accuracy: 0.9862\n",
      "Epoch 27/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0203 - accuracy: 0.9940 - val_loss: 0.0459 - val_accuracy: 0.9860\n",
      "Epoch 28/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 0.0486 - val_accuracy: 0.9868\n",
      "Epoch 29/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0203 - accuracy: 0.9937 - val_loss: 0.0459 - val_accuracy: 0.9869\n",
      "Epoch 30/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.0528 - val_accuracy: 0.9859\n",
      "Epoch 31/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.0489 - val_accuracy: 0.9862\n",
      "Epoch 32/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0184 - accuracy: 0.9942 - val_loss: 0.0493 - val_accuracy: 0.9870\n",
      "Epoch 33/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0187 - accuracy: 0.9943 - val_loss: 0.0448 - val_accuracy: 0.9885\n",
      "Epoch 34/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.0472 - val_accuracy: 0.9890\n",
      "Epoch 35/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0182 - accuracy: 0.9945 - val_loss: 0.0465 - val_accuracy: 0.9873\n",
      "Epoch 36/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0171 - accuracy: 0.9949 - val_loss: 0.0489 - val_accuracy: 0.9873\n",
      "Epoch 37/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.0502 - val_accuracy: 0.9868\n",
      "Epoch 38/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0167 - accuracy: 0.9952 - val_loss: 0.0496 - val_accuracy: 0.9871\n",
      "Epoch 39/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0164 - accuracy: 0.9951 - val_loss: 0.0492 - val_accuracy: 0.9873\n",
      "Epoch 40/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.0488 - val_accuracy: 0.9874\n",
      "Epoch 41/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0514 - val_accuracy: 0.9874\n",
      "Epoch 42/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0165 - accuracy: 0.9952 - val_loss: 0.0464 - val_accuracy: 0.9877\n",
      "Epoch 43/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.0468 - val_accuracy: 0.9882\n",
      "Epoch 44/65\n",
      "2100/2100 [==============================] - 29s 14ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.0455 - val_accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=65, validation_data=val_ds, callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07dd8a2",
   "metadata": {},
   "source": [
    "## Visualizing the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5804d87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41c910",
   "metadata": {},
   "source": [
    "### Model Training with Full Dataset \n",
    "In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd06753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model building\n",
    "model_full = keras.models.Sequential()\n",
    "\n",
    "model_full.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', input_shape=input_shape_notFlattened))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Activation(activation_fn))\n",
    "model_full.add(keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'))\n",
    "model_full.add(keras.layers.BatchNormalization())\n",
    "model_full.add(keras.layers.Activation(activation_fn))\n",
    "model_full.add(keras.layers.Flatten())\n",
    "model_full.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rt)\n",
    "\n",
    "model_full.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5827a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                62730     \n",
      "=================================================================\n",
      "Total params: 137,994\n",
      "Trainable params: 137,610\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "853576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new log dir for tensorboard\n",
    "tensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "checkpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_modell_full.h5\", save_best_only=False, save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35f66013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing full features set (X) for the tensorflow data api\n",
    "\n",
    "training_dataset_all = training_dataset.concatenate(training_crop_dataset)\n",
    "val_dataset_all = val_dataset.concatenate(val_crop_dataset)\n",
    "\n",
    "training_ds_all = training_dataset_all.concatenate(val_dataset_all)\n",
    "\n",
    "training_ds_all = training_ds_all.shuffle(20000).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86642c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   2/2625 [..............................] - ETA: 9:30 - loss: 2.7900 - accuracy: 0.1719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.4210s). Check your callbacks.\n",
      "2625/2625 [==============================] - 33s 12ms/step - loss: 0.2064 - accuracy: 0.9378\n",
      "Epoch 2/30\n",
      "2625/2625 [==============================] - 32s 12ms/step - loss: 0.0924 - accuracy: 0.9719\n",
      "Epoch 3/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0726 - accuracy: 0.9772\n",
      "Epoch 4/30\n",
      "2625/2625 [==============================] - 35s 14ms/step - loss: 0.0597 - accuracy: 0.9815\n",
      "Epoch 5/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0507 - accuracy: 0.9847\n",
      "Epoch 6/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0446 - accuracy: 0.9864\n",
      "Epoch 7/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0401 - accuracy: 0.9874\n",
      "Epoch 8/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0374 - accuracy: 0.9887\n",
      "Epoch 9/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0349 - accuracy: 0.9894\n",
      "Epoch 10/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0325 - accuracy: 0.9902\n",
      "Epoch 11/30\n",
      "2625/2625 [==============================] - 35s 13ms/step - loss: 0.0290 - accuracy: 0.9911\n",
      "Epoch 12/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0290 - accuracy: 0.9914\n",
      "Epoch 13/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0280 - accuracy: 0.9915\n",
      "Epoch 14/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0256 - accuracy: 0.9921\n",
      "Epoch 15/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0268 - accuracy: 0.9918\n",
      "Epoch 16/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0243 - accuracy: 0.9925\n",
      "Epoch 17/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0236 - accuracy: 0.9927\n",
      "Epoch 18/30\n",
      "2625/2625 [==============================] - 33s 13ms/step - loss: 0.0240 - accuracy: 0.9928\n",
      "Epoch 19/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0229 - accuracy: 0.9929\n",
      "Epoch 20/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0224 - accuracy: 0.9932\n",
      "Epoch 21/30\n",
      "2625/2625 [==============================] - 33s 13ms/step - loss: 0.0215 - accuracy: 0.9937\n",
      "Epoch 22/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0205 - accuracy: 0.9938\n",
      "Epoch 23/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0211 - accuracy: 0.9937\n",
      "Epoch 24/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0202 - accuracy: 0.9939\n",
      "Epoch 25/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0194 - accuracy: 0.9943\n",
      "Epoch 26/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0190 - accuracy: 0.9938\n",
      "Epoch 27/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0197 - accuracy: 0.9940\n",
      "Epoch 28/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0195 - accuracy: 0.9943\n",
      "Epoch 29/30\n",
      "2625/2625 [==============================] - 34s 13ms/step - loss: 0.0184 - accuracy: 0.9948\n",
      "Epoch 30/30\n",
      "2625/2625 [==============================] - 33s 13ms/step - loss: 0.0184 - accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \n",
    "history_full = model_full.fit(training_ds_all, epochs=30, callbacks=[tensorboard_cb_f, checkpoint_cb_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "486441b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history_full.history))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42cae8",
   "metadata": {},
   "source": [
    "# Image Prediction of Unknown Data (Test Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d29b5",
   "metadata": {},
   "source": [
    "## Peparing Test Data\n",
    "As well as previously done, we need to create a TF dataset of the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "188df591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe format into tensorflow compatible format.\n",
    "X_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(X_test, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a905769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3e08e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960885ba",
   "metadata": {},
   "source": [
    "## Creating Competition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b533ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c80eb9",
   "metadata": {},
   "source": [
    "## Prediction of Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9ae3b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAADfCAYAAADr9A+kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANIUlEQVR4nO3de7CUdR3H8c/XIzcRM0AQFUEddbCLZIxQmuXgLSLBsrIpOzka1URTTTUxdLMZp6msrOliWqHYRSkSpRknL0xjNy3BkEt495QIciAp6QLC4dsf5zl5xP2dy7PPPs+X3fdr5szuPt+z+3zd44dn93l2n6+5uwBU64CqGwBAEIEQCCIQAEEEAiCIQAAEEQjgwHrubGbnSfqmpDZJP3D3L/X1+0NtmA/XyHpWCey3dmj7Nnc/rFYtdxDNrE3SdySdLWmjpPvMbLm7/yV1n+Eaqek2M+8qgf3aXb70r6laPS9NT5X0qLs/7u7PSbpJ0pw6Hg9oWfUE8UhJT/a6vTFb9gJmNs/MVprZyt3aVcfqgOZVTxCtxrIXfV7O3a9192nuPm2IhtWxOqB51RPEjZIm9rp9lKRN9bUDtKZ6gnifpOPN7BgzGyrpIknLi2kLaC2595q6+x4zmy/pdnUfvljk7usL6wxoIXUdR3T32yTdVlAvQMvikzVAAAQRCIAgAgEQRCAAgggEQBCBAAgiEABBBAIgiEAABBEIgCACARBEIACCCARAEIEACCIQAEEEAiCIQAAEEQigrlNloH5thxxSc7kdNKLUPjpnHZusjXn33wb9ePax2v9dkrT3gQ2DfrxmxxYRCIAgAgEQRCAAgggEQBCBAAgiEEC9E4M7JO2Q1CVpj7tPK6KpVrLhyhNrLn949vdK7qRYsw69LFnjX/8XK+I44pnuvq2AxwFaFv84AQHUG0SXdIeZrTKzeUU0BLSiel+anubum8xsnKQ7zexBd/9N71/IAjpPkobroDpXBzSnuraI7r4pu+yUtEzSqTV+h9HdQD9yB9HMRprZqJ7rks6RtK6oxoBWUs9L0/GSlplZz+P81N1/VUhXTWbn7Be9UPi/a2ZeV2In5Xn9t+5J1p7e9ZJk7aGPTUnWDvjd6npaCq2e0d2PSzq5wF6AlsXhCyAAgggEQBCBAAgiEABBBALg5FEluPDLtydrZ47YWWIn5fnUmPW57rd8UfrEUt/94NuStQNXrMq1vijYIgIBEEQgAIIIBEAQgQAIIhAAQQQC4PBFCZZ87rxk7eQrr6m5/DXDugrv4+SrP5ysHX37jlyP+cT5B9dcvqL9yuR9xrel53qcP3J7svbJt6T/dz3h7nTN9+xJ1qJgiwgEQBCBAAgiEABBBAIgiEAABBEIwNy9tJUdYqN9us0sbX37g//OrX1iqc5T2gpf1+RlzyZr/ud835ZImfHA7mTtM2PXFLouSZozNX2IqGvr1sLXl8ddvnRVaj4MW0QgAIIIBEAQgQAIIhAAQQQCIIhAAP1++8LMFkmaLanT3V+eLRstaYmkyZI6JL3d3dMfm0fSiFv+VHP5pFuKX1d5B6qkuxe8Nln7zA+KP3yxvxvIFvF6SfsepFkgaYW7Hy9pRXYbQE79BjEbPPrMPovnSFqcXV8saW6xbQGtJe97xPHuvlmSsstxqV80s3lmttLMVu7WrpyrA5pbw3fWMDEY6F/eIG4xswmSlF12FtcS0HrynrNmuaR2SV/KLm8trCM0hWHbeRsyGP1uEc3sRkn3SDrRzDaa2aXqDuDZZvaIpLOz2wBy6neL6O7vTJT4PhNQED5ZAwRAEIEACCIQAEEEAuCU+2iIp2fUPhU/amOLCARAEIEACCIQAEEEAiCIQAAEEQiAwxdoiLmX3F11C/sVtohAAAQRCIAgAgEQRCAAgggEQBCBADh8sR/a+ebaU4Yl6ZkT03/SA7rSj3n4VX/I1YufNrXm8lcdtDTX4/Vl/lOnp4u79u+TVbFFBAIgiEAABBEIgCACARBEIACCCASQd2Lw5ZLeJ2lr9msL3f22RjVZprZDX5Ks2eiXJmsd7zgiWRuxNT2r94RLHhxYY728d/x1ydqZI3Yma7s9ffzisgvPHXQfknTOmNp/9jcd9M9cj/eN7Scka0++a0Ky1vXs47nWF0XeicGSdJW7T81+miKEQFXyTgwGUKB63iPON7M1ZrbIzJKv2ZgYDPQvbxCvlnScpKmSNkv6WuoXmRgM9C9XEN19i7t3ufteSd+XlP7wI4B+5Qpiz9juzAWS1hXTDtCaBnL44kZJb5A01sw2Svq8pDeY2VRJLqlD0vsb12JOM16ZLHXMHpmsHTZtS7L261f8vK6WqjbE2pK1xZPvKrGTtIlD0vsFH2sfn6wd+8Wnk7W9//lPXT2VIe/E4B82oBegZfHJGiAAgggEQBCBAAgiEABBBAJo2pNHPXF++hDF+vZvl9iJtK3rv8nakh0vr7n8iCHbk/e5YGTzfvT3rQdvS9cuSf/dpk55T7I26QOdyVrX1q3JWpnYIgIBEEQgAIIIBEAQgQAIIhBA0+413dD+nWRtbwPW195xVrK2dtmUZO2Ir9Y+1X3by6Yn77Pqxw8la1eMW5Ws5fXEnvR5cN500ycG/XjTX7chWbtu0opBP54krZ5xQ7I288cXJmsjzmWvKYAMQQQCIIhAAAQRCIAgAgEQRCAAc0+fDr5oh9hon24zS1nX7ZtWJ2t9nXo+r4d3P5esrX/u8ELX9ephTyVrRx84Itdj/n7nkGRt4cJ5ydqoJfcOel0HHp4+98y/b0j3/9njfpmsnTE8/fz3ZfaRr851vzzu8qWr3H1arRpbRCAAgggEQBCBAAgiEABBBAIgiEAAAznl/kRJN0g6XN1fXLjW3b9pZqMlLZE0Wd2n3X+7u6dPtFKyKb+/OFlb89rrC1/fCUOG9lEr+hwz6V38V2xLjxpYuuT1ydroB9OHdEbdPPhDFH3Z83R6rMGwc9L3+8KcS5O1n37r68naWfd+MFmbpLXpFZZoIFvEPZI+7u5TJM2Q9CEzO0nSAkkr3P14SSuy2wByGMjE4M3ufn92fYekDZKOlDRH0uLs1xZLmtugHoGmN6j3iGY2WdKrJP1R0nh33yx1h1XSuMR9mBgM9GPAQTSzgyX9QtJH3f3Zgd6PicFA/wYURDMbou4Q/sTdb84Wb+kZWJpdps/iCqBP/QbRzEzd8xA3uHvvXVPLJbVn19sl3Vp8e0Br6PfbF2Z2uqTfSlqr58+7tFDd7xN/JuloSX+T9DZ373M/fZnfvjhg+PBkzY6akKx1XbO7Ee0MWtv8Pr5Fse0f6dqu9PvwrmcH/I5iv9M2dkyy5v/6d7K2d2f6xFhF6+vbFwOZGPw7SZYol5MqoMnxyRogAIIIBEAQgQAIIhAAQQQCaNrZF33uln70iXQtyH7g4k9v1dy6tv296hbqwhYRCIAgAgEQRCAAgggEQBCBAAgiEABBBAIgiEAABBEIgCACARBEIACCCARAEIEACCIQAEEEAiCIQAAEEQiAIAIBEEQgAIIIBDCQITQTzezXZrbBzNab2Uey5Zeb2VNmtjr7mdX4doHmNJCzuPWM7r7fzEZJWmVmd2a1q9z9q41rD2gNAxlCs1lSz2TgHWbWM7obQEHqGd0tSfPNbI2ZLTKzlybuw+huoB/1jO6+WtJxkqaqe4v5tVr3Y3Q30L/co7vdfYu7d7n7Xknfl3Rq49oEmlvu0d1m1nvs7gWS1hXfHtAaBrLX9DRJF0taa2ars2ULJb3TzKZKckkdkt7fgP6AllDP6O7bim8HaE18sgYIgCACARBEIACCCARAEIEACCIQAEEEAiCIQAAEEQiAIAIBEEQgAHP38lZmtlXSX7ObYyVtK23lfYvSC328UJQ+pGJ6meTuh9UqlBrEF6zYbKW7T6tk5fuI0gt9xOxDanwvvDQFAiCIQABVBvHaCte9ryi90McLRelDanAvlb1HBPA8XpoCARBEIIBKgmhm55nZQ2b2qJktqKKHrI8OM1ubze5YWfK6F5lZp5mt67VstJndaWaPZJc1T9pcQh+lzzXpY8ZKqc9JZbNe3L3UH0ltkh6TdKykoZIekHRS2X1kvXRIGlvRus+QdIqkdb2WfUXSguz6AklfrqiPyyV9ouTnY4KkU7LroyQ9LOmksp+TPvpo6HNSxRbxVEmPuvvj7v6cpJskzamgj0q5+28kPbPP4jmSFmfXF0uaW1EfpXP3ze5+f3Z9h6SeGSulPid99NFQVQTxSElP9rq9UdUNtXFJd5jZKjObV1EPvY337qE/yi7HVdhLv3NNGmWfGSuVPSd5Zr3kVUUQa50jtapjKKe5+ymS3ijpQ2Z2RkV9RDOguSaNUGPGSiXyznrJq4ogbpQ0sdftoyRtqqAPufum7LJT0jJVP79jS88og+yys4omvKK5JrVmrKiC56SKWS9VBPE+Sceb2TFmNlTSRZKWl92EmY3MBq/KzEZKOkfVz+9YLqk9u94u6dYqmqhirklqxopKfk4qm/VS5p6xXnumZql7b9Rjkj5dUQ/HqnuP7QOS1pfdh6Qb1f0SZ7e6XyVcKmmMpBWSHskuR1fUx48krZW0Rt1BmFBCH6er+y3KGkmrs59ZZT8nffTR0OeEj7gBAfDJGiAAgggEQBCBAAgiEABBBAIgiEAABBEI4H/odxOCcaKRfAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the image\n",
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch in test_ds.take(1):\n",
    "    for index in range(1):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e536cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propability of all lables for given pixels:  [5.5420935e-14 1.1841813e-16 1.0000000e+00 1.7775965e-11 1.8289284e-13\n",
      " 7.8190480e-18 1.0913715e-18 4.1816822e-15 2.1135141e-15 3.2843567e-15]\n"
     ]
    }
   ],
   "source": [
    "for element in test_ds.take(1):\n",
    "    print(\"Propability of all lables for given pixels: \", model_full.predict(test_ds.take(1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4f98453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Digit: \",np.argmax(model_full.predict(test_ds.take(1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52d3b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_full.predict(test_ds)                                                                           # predict the probability\n",
    "predictions = np.argmax(predictions, axis=1)                                                                        # getting the predicted digit numbers based ont the probability of every np element \n",
    "mnist_competition_file = pd.DataFrame(predictions)                                                                  # converting into df\n",
    "mnist_competition_file.index += 1                                                                                   # index should start at 1\n",
    "mnist_competition_file.reset_index(level=0, inplace=True)                                                           # make the index a column \n",
    "mnist_competition_file = mnist_competition_file.rename(columns={\"index\": \"ImageId\", 0: \"Label\"}, errors=\"raise\")    # renamen them according to the competition requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9c5eec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_competition_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12d88fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\n",
    "mnist_competition_file.Label = mnist_competition_file.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "beb2d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_competition_file.to_csv('mnist_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5974e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b49f70aa2f17b03439dc8f4bbaf601f728142d0d0d774f4bbd10ea7a16b86ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('wingpuflake_keras': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
